seed_everything: true
trainer:
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  devices: 8
  num_nodes: 1
  precision: 32
  max_epochs: 50
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 0
  log_every_n_steps: 100
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "vq_audio_simvq_wav_big/8k_ration_20_loss" # Please specify your own path
        save_top_k: -1 # save all checkpoints
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: "vq_audio_simvq_wav_big/8k_ration_20_loss" #Please specify your own path
      version: "1second"
      name: "tmp"

model:
  class_path: taming.models.vq_audio_simvq.VQModel
  init_args:
    ddconfig:
      causal: true
      dimension: 512
      ratios: [8,5,5,4]
    
    lossconfig:
      target: taming.modules.losses.stft_simvq.VQSTFTWithDiscriminator
      params:
        disc_conditional: False
        disc_in_channels: 1
        disc_start: 0 # from 0 epoch
        codebook_enlarge_ratio: 0
        codebook_enlarge_steps: 2000
        sample_rate: 16000
        commit_weight: 1000.0
        gen_loss_weight: 1.0
        mel_loss_coeff: 45.0
        mrd_loss_coeff: 1.0
    
    sample_rate: 16000
    audio_normalize: false
    segment: None
    learning_rate: 1e-4
    scheduler_type: "None"
    use_ema: true

data:
  class_path: taming.data.speechtokenizer.SpeechTokenizerDataModule
  init_args:
    batch_size: 18
    num_workers: 8
    train_path : "/mnt/nfs3/zhangjinouwen/dataset/rep/rep_big_hubert_train.txt"
    val_path : "/mnt/nfs3/zhangjinouwen/dataset/rep/rep_big_hubert_eval.txt"

ckpt_path: /root/Github/TTS_Tokenizer/SimVQ/vq_audio_simvq_wav_big/8k_ration_20_loss/epoch=20-step=203301.ckpt
